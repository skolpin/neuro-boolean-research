{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ec5eb2-3cbe-4652-9d78-2bb72d039795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e93260-1815-4f14-8723-b6ce10d3ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bin(x, n=7):\n",
    "    \"\"\"\n",
    "    creating a binary list of integer non-negative x\n",
    "    \"\"\"\n",
    "    u = 2 ** n - 1\n",
    "    x = int(x)\n",
    "    assert x >= 0, 'Input value x must be non-negative'\n",
    "    assert x <= u, f'Input value x with n = {n} must be less than {u}'\n",
    "    \n",
    "    y = []\n",
    "    if x == 0:\n",
    "        for i in range(n):\n",
    "            y.append(0)\n",
    "    else:\n",
    "        while x != 1:\n",
    "            y.append(x % 2)\n",
    "            x = x // 2\n",
    "        y.append(x)\n",
    "        delta = n - len(y)\n",
    "        for i in range(delta):\n",
    "            y.append(0)\n",
    "        y.reverse()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db837d9c-04b2-41e5-8ee4-e615959b4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_X(n_inputs):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    all binary combinations of these variables\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for i in range(2**n_inputs):\n",
    "        X += [dec2bin(i, n=n_inputs)]\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a2cedd-519d-4d14-bce2-4e612aa608d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_boolfunc(n_inputs):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    the truth table, where output is random binary vector\n",
    "    \"\"\"\n",
    "    X = get_all_X(n_inputs)\n",
    "    y = np.random.randint(0, 2, size=(2**n_inputs, 1))\n",
    "    return X, y\n",
    "   \n",
    "def _and(n_inputs=2):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    the truth table of AND logical gate\n",
    "    \"\"\"\n",
    "    X = get_all_X(n_inputs)\n",
    "    y = np.append(\n",
    "        np.zeros(shape=(2 ** n_inputs - 1, 1), dtype=int),\n",
    "        np.array([[1]]),\n",
    "        axis=0\n",
    "    )\n",
    "    return X, y\n",
    "    \n",
    "def _or(n_inputs=2):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    the truth table of OR logical gate\n",
    "    \"\"\"\n",
    "    X = get_all_X(n_inputs)\n",
    "    y = np.append(\n",
    "        np.array([[0]]),\n",
    "        np.ones(shape=(2 ** n_inputs - 1, 1), dtype=int),\n",
    "        axis=0\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "def _xor(n_inputs=2):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    the truth table of XOR logical gate\n",
    "    \"\"\"\n",
    "    X = get_all_X(n_inputs)\n",
    "    y = (np.sum(X, axis=1) % 2).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def get_certain_boolfunc(function_number, n_inputs):\n",
    "    \"\"\"\n",
    "    for given function_number and number of variables returns\n",
    "    the truth table, where output is binary representation of function_number\n",
    "    \n",
    "    NOTE: function_number MUST BE NOT MORE THAN 2^(2^n_inputs)-1\n",
    "    \"\"\"\n",
    "    X = get_all_X(n_inputs)\n",
    "    try:\n",
    "        y = np.array(dec2bin(function_number, 2 ** n_inputs)).reshape(-1, 1)\n",
    "        return X, y\n",
    "    except AssertionError:\n",
    "        raise ValueError(f'Given function_number = {function_number} is more than 2 ^ (2 ^ n_inputs) - 1 = {2 ** (2 ** n_inputs) - 1}.')\n",
    "        \n",
    "\n",
    "def to_dataframe(X, y):\n",
    "    \"\"\"\n",
    "    for the truth table in form of two arrays \n",
    "    X [2 ** n_inputs, n_inputs] and y [2 ** n_inputs, 1]\n",
    "    combine it to the form of Pandas DataFrame\n",
    "    \"\"\"\n",
    "    data=np.concatenate((X, y), axis=1)\n",
    "    n_inputs = X.shape[1]\n",
    "    return pd.DataFrame(data=data, columns=[f'x{i}' for i in range(n_inputs, 0, -1)] + ['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a150877b-7eef-4b6e-bedc-a34690fa7f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.86 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x2</th>\n",
       "      <th>x1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x2  x1  y\n",
       "0   0   0  0\n",
       "1   0   1  1\n",
       "2   1   0  1\n",
       "3   1   1  0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n = 2\n",
    "# set_random_seed(23) # just for get a complex random function\n",
    "# X, y = random_boolfunc(n_inputs=n)\n",
    "X, y = _xor(2)\n",
    "to_dataframe(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e34c8fc-4244-4097-9ff5-cb6caad866fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = torch.Tensor(X), torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1415d0e8-0837-47da-b1b7-ce2121b95b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(nn.Module):\n",
    "    def __init__(self, n_inputs, activation=nn.Sigmoid(), init_form='normal'):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_inputs, 1)\n",
    "        self.activation = activation\n",
    "        self.init_form = init_form\n",
    "        if init_form is not None:\n",
    "            self.init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "    def init(self):\n",
    "        if self.init_form == 'normal':\n",
    "            nn.init.xavier_normal_(self.fc.weight, gain=nn.init.calculate_gain('sigmoid'))\n",
    "            nn.init.zeros_(self.fc.bias)\n",
    "        elif self.init_form == 'uniform':\n",
    "            nn.init.xavier_uniform_(self.fc.weight, gain=nn.init.calculate_gain('sigmoid'))\n",
    "            nn.init.zeros_(self.fc.bias)\n",
    "        else:\n",
    "            print('incorrect init_form')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9b18cfd-6a88-4ba1-b03c-c3332a7573db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, epochs, device, optimizer='Adam', \n",
    "          criterion=torch.nn.BCELoss(), verbose=True):\n",
    "    \n",
    "    len_dataset = X.shape[0]\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=len_dataset, shuffle=True)\n",
    "    \n",
    "    if optimizer=='Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    else:\n",
    "        print('other optimizers does not supported yet')\n",
    "    \n",
    "    log_epoch = epochs // 10\n",
    "    for i in range(1, epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X.to(device))\n",
    "            loss = criterion(output, y.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().cpu().item()\n",
    "        epoch_loss /= len(dataloader)\n",
    "        if verbose and i % log_epoch == 0:\n",
    "            print(f'epoch {i}: loss {epoch_loss:.2f}')\n",
    "\n",
    "def step(x):\n",
    "    return torch.heaviside(x - 0.5, torch.tensor([[1.]]))\n",
    "\n",
    "def check(model, X, y, device, verbose=True):\n",
    "    out = model(X.to(device))\n",
    "    step_out = step(out.to('cpu'))\n",
    "    mask = (step_out != y)[:,0]  \n",
    "    \n",
    "    if any(mask):\n",
    "        wrong_ans_numbers = [i for i in range(mask.shape[0]) if mask[i].item()]\n",
    "        if verbose:\n",
    "            print(f'{type(model).__name__} gives wrong answers for samples {wrong_ans_numbers}')\n",
    "        return X[mask], y[mask]\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('Success!')\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d060a324-ea67-4370-940b-265519f71416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500: loss 0.69\n",
      "epoch 1000: loss 0.69\n",
      "epoch 1500: loss 0.69\n",
      "epoch 2000: loss 0.69\n",
      "epoch 2500: loss 0.69\n",
      "epoch 3000: loss 0.69\n",
      "epoch 3500: loss 0.69\n",
      "epoch 4000: loss 0.69\n",
      "epoch 4500: loss 0.69\n",
      "epoch 5000: loss 0.69\n"
     ]
    }
   ],
   "source": [
    "model = Neuron(n_inputs=n).to(device)\n",
    "\n",
    "train(model, X, y, device=device, epochs=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb40d52-6b8f-4777-b193-09860423363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron gives wrong answers for samples [0, 3]\n"
     ]
    }
   ],
   "source": [
    "X_new, y_new = check(model, X, y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07229066-7fd3-4a62-9794-3de127efeb76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500: loss 0.03\n",
      "epoch 1000: loss 0.01\n",
      "epoch 1500: loss 0.01\n",
      "epoch 2000: loss 0.00\n",
      "epoch 2500: loss 0.00\n",
      "epoch 3000: loss 0.00\n",
      "epoch 3500: loss 0.00\n",
      "epoch 4000: loss 0.00\n",
      "epoch 4500: loss 0.00\n",
      "epoch 5000: loss 0.00\n"
     ]
    }
   ],
   "source": [
    "model2 = Neuron(n_inputs=n).to(device)\n",
    "\n",
    "train(model2, X_new, y_new, device=device, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc0a0467-8d86-4c41-8a59-bef47f70c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "X_new2, y_new2 = check(model2, X_new, y_new, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a635c85e-69f4-47b8-b70f-4a137fa53568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# https://github.com/pytorch/pytorch/issues/36459#:~:text=Here%27s%20my%20definition%3A\n",
    "\n",
    "class BigModel(nn.Module):\n",
    "    def __init__(self, base_models : list, activation=nn.Sigmoid(), init_form='normal'):\n",
    "        super().__init__()\n",
    "        self.base_models = nn.ModuleList(deepcopy(base_models))\n",
    "        \n",
    "        # for model in self.base_models:\n",
    "        #     for param in model.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        \n",
    "        self.n_hidden = len(base_models)\n",
    "        self.fc = nn.Linear(self.n_hidden, 1)\n",
    "        self.activation = activation\n",
    "        self.merge = lambda x: torch.cat(x, dim=1)\n",
    "        self.init_form = init_form\n",
    "        if init_form is not None:\n",
    "            self.init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden_out = self.merge([module(x) for module in self.base_models])\n",
    "        out = self.activation(self.fc(hidden_out))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init(self):\n",
    "        if self.init_form == 'normal':\n",
    "            nn.init.xavier_normal_(self.fc.weight, gain=nn.init.calculate_gain('sigmoid'))\n",
    "            nn.init.zeros_(self.fc.bias)\n",
    "        elif self.init_form == 'uniform':\n",
    "            nn.init.xavier_uniform_(self.fc.weight, gain=nn.init.calculate_gain('sigmoid'))\n",
    "            nn.init.zeros_(self.fc.bias)\n",
    "        else:\n",
    "            print('incorrect init_form')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a51e9cee-cedd-4296-b7f1-11c301a8772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500: loss 0.17\n",
      "epoch 1000: loss 0.03\n",
      "epoch 1500: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "epoch 2500: loss 0.00\n",
      "epoch 3000: loss 0.00\n",
      "epoch 3500: loss 0.00\n",
      "epoch 4000: loss 0.00\n",
      "epoch 4500: loss 0.00\n",
      "epoch 5000: loss 0.00\n"
     ]
    }
   ],
   "source": [
    "big_model = BigModel([model, model2]).to(device)\n",
    "\n",
    "train(big_model, X, y, device=device, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecd01678-225b-4f86-a1e0-02a2060b9752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(big_model, X, y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c450d6ea-ba3c-4d41-b257-3541224bfd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 1/10 [00:02<00:21,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential gives wrong answers for samples [2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 2/10 [00:04<00:19,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential gives wrong answers for samples [1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████▏                              | 3/10 [00:07<00:17,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 4/10 [00:09<00:14,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 5/10 [00:12<00:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████▍                 | 6/10 [00:14<00:09,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████▊             | 7/10 [00:17<00:07,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential gives wrong answers for samples [2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████▏        | 8/10 [00:19<00:04,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential gives wrong answers for samples [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████▌    | 9/10 [00:21<00:02,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential gives wrong answers for samples [2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "{'success': 5, 'fail': 5}\n",
      "CPU times: user 24.3 s, sys: 790 µs, total: 24.3 s\n",
      "Wall time: 24.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter = {'success':0, 'fail':0}\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "\n",
    "    mlp = nn.Sequential(\n",
    "        nn.Linear(n,2),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(2,1),\n",
    "        nn.Sigmoid()\n",
    "    ).to(device)\n",
    "    \n",
    "    nn.init.xavier_normal_(mlp[0].weight, 1.0)\n",
    "    nn.init.zeros_(mlp[0].bias)\n",
    "    nn.init.xavier_normal_(mlp[2].weight, 1.0)\n",
    "    nn.init.zeros_(mlp[2].bias)\n",
    "    \n",
    "\n",
    "    train(mlp, X, y, device=device, epochs=5000, verbose=False)\n",
    "\n",
    "    if check(mlp, X, y, device=device)[0] is None:\n",
    "        counter['success'] += 1\n",
    "    else:\n",
    "        counter['fail'] += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0995fa10-6298-45cf-a7fd-382d7540612e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x4</th>\n",
       "      <th>x3</th>\n",
       "      <th>x2</th>\n",
       "      <th>x1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x4  x3  x2  x1  y\n",
       "0    0   0   0   0  0\n",
       "1    0   0   0   1  1\n",
       "2    0   0   1   0  1\n",
       "3    0   0   1   1  0\n",
       "4    0   1   0   0  1\n",
       "5    0   1   0   1  0\n",
       "6    0   1   1   0  0\n",
       "7    0   1   1   1  1\n",
       "8    1   0   0   0  1\n",
       "9    1   0   0   1  0\n",
       "10   1   0   1   0  0\n",
       "11   1   0   1   1  1\n",
       "12   1   1   0   0  0\n",
       "13   1   1   0   1  1\n",
       "14   1   1   1   0  1\n",
       "15   1   1   1   1  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 4\n",
    "# set_random_seed(23) # just for get a complex random function\n",
    "# X, y = random_boolfunc(n_inputs=n)\n",
    "X, y = _xor(n)\n",
    "display(to_dataframe(X, y))\n",
    "X, y = torch.Tensor(X), torch.Tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f6f478d-243b-4740-a679-f2ad989233ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron gives wrong answers for samples [0, 3, 5, 6, 9, 10, 12, 15]\n",
      "Success!\n",
      "BigModel gives wrong answers for samples [7, 11, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "neurons = []\n",
    "current_neuron = Neuron(n_inputs=n, activation=nn.Sigmoid()).to(device)\n",
    "\n",
    "train(current_neuron, X, y, device=device, epochs=5000, verbose=False)\n",
    "\n",
    "X_new, y_new = check(current_neuron, X, y, device=device)\n",
    "\n",
    "if X_new is None:\n",
    "    result_model = current_neuron\n",
    "    print(result_model)\n",
    "\n",
    "else:\n",
    "    neurons.append(deepcopy(current_neuron))\n",
    "    \n",
    "    while X_new is not None:\n",
    "        \n",
    "        current_neuron = Neuron(n_inputs=n, activation=nn.Sigmoid()).to(device)\n",
    "\n",
    "        train(current_neuron, X_new, y_new, device=device, epochs=5000, verbose=False)\n",
    "\n",
    "        X_new, y_new = check(current_neuron, X_new, y_new, device=device)\n",
    "        \n",
    "        neurons.append(deepcopy(current_neuron))\n",
    "    \n",
    "    # neurons.append(Neuron(n_inputs=n, activation=nn.ReLU()))\n",
    "    # neurons.append(Neuron(n_inputs=n))\n",
    "    \n",
    "    big_model = BigModel(neurons, activation=nn.Sigmoid()).to(device)\n",
    "    \n",
    "    train(big_model, X, y, device=device, epochs=100000, verbose=False)\n",
    "    \n",
    "    check(big_model, X, y, device=device)\n",
    "    \n",
    "    result_model = deepcopy(big_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "961c59ab-30f0-4bf3-8837-056ddea5af84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigModel gives wrong answers for samples [7, 11, 13, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 1., 0., 1.],\n",
       "         [1., 1., 1., 0.]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(result_model, X, y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aa16a1d-981a-4c1c-9131-b1020adb5d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigModel(\n",
       "  (base_models): ModuleList(\n",
       "    (0): Neuron(\n",
       "      (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "    (1): Neuron(\n",
       "      (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93850902-89f4-4170-ad50-27121d8b4d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3636]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model(X[15:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f578c6-a1bf-4a5c-84fb-b6e3e2c44d87",
   "metadata": {},
   "source": [
    "# Эксперимент 1. XOR(4) vs Algorithm\n",
    "Учим XOR(4) по алгоритму. З модели, каждая учится 100 раз\n",
    "* из 2 предобученных нейронов\n",
    "* 2 предобученных + 1 случайный (экстра-нейрон)\n",
    "* 2 предобученных + 2 случайных\n",
    "* 2 предобученных + 3 случайных\n",
    "\n",
    "Также для сравнения перебираем три формы инициализации: дефолтную, Ксавье-нормальную, Ксавье-равномерную "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90e18854-d0ef-4aa0-88ab-fb86593f0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_1(X, y, init_form, num_extra_neurons, num_runs):\n",
    "    counter = {'success':0, 'fail':0}\n",
    "\n",
    "    for _ in tqdm(range(num_runs)):\n",
    "\n",
    "        neurons = []\n",
    "        current_neuron = Neuron(n_inputs=n, activation=nn.Sigmoid(), init_form=init_form).to(device)\n",
    "\n",
    "        train(current_neuron, X, y, device=device, epochs=5000, verbose=False)\n",
    "\n",
    "        X_new, y_new = check(current_neuron, X, y, device=device, verbose=False)\n",
    "\n",
    "        if X_new is None:\n",
    "            result_model = current_neuron\n",
    "            print(result_model)\n",
    "\n",
    "        else:\n",
    "            neurons.append(deepcopy(current_neuron))\n",
    "\n",
    "            while X_new is not None:\n",
    "\n",
    "                current_neuron = Neuron(n_inputs=n, activation=nn.Sigmoid(), init_form=init_form).to(device)\n",
    "\n",
    "                train(current_neuron, X_new, y_new, device=device, epochs=5000, verbose=False)\n",
    "\n",
    "                X_new, y_new = check(current_neuron, X_new, y_new, device=device, verbose=False)\n",
    "\n",
    "                neurons.append(deepcopy(current_neuron))\n",
    "\n",
    "            for extra in range(num_extra_neurons):\n",
    "                neurons.append(Neuron(n_inputs=n, activation=nn.Sigmoid(), init_form=init_form).to(device))\n",
    "\n",
    "            big_model = BigModel(neurons, activation=nn.Sigmoid(), init_form=init_form).to(device)\n",
    "\n",
    "            train(big_model, X, y, device=device, epochs=100000, verbose=False)\n",
    "\n",
    "            check(big_model, X, y, device=device, verbose=False)\n",
    "\n",
    "            result_model = deepcopy(big_model)\n",
    "\n",
    "        if check(result_model, X, y, device=device, verbose=False)[0] is None:\n",
    "            counter['success'] += 1\n",
    "        else:\n",
    "            counter['fail'] += 1\n",
    "        \n",
    "    return result_model, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f718e35-2bc9-410e-872f-befaa25e7763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None init\n",
      "    0 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [1:49:49<00:00, 65.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 0, 'fail': 100}\n",
      "    1 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [2:05:59<00:00, 75.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 39, 'fail': 61}\n",
      "    2 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [2:22:06<00:00, 85.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 88, 'fail': 12}\n",
      "    3 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [2:38:17<00:00, 94.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 98, 'fail': 2}\n",
      "normal init\n",
      "    0 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [1:50:03<00:00, 66.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 0, 'fail': 100}\n",
      "    1 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [2:06:06<00:00, 75.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 55, 'fail': 45}\n",
      "    2 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [2:22:09<00:00, 85.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 85, 'fail': 15}\n",
      "    3 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [2:42:16<00:00, 97.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 93, 'fail': 7}\n",
      "uniform init\n",
      "    0 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [1:53:28<00:00, 68.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 0, 'fail': 100}\n",
      "    1 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [2:06:35<00:00, 75.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 52, 'fail': 48}\n",
      "    2 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [2:23:09<00:00, 85.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 85, 'fail': 15}\n",
      "    3 extra neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [2:37:54<00:00, 94.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {'success': 88, 'fail': 12}\n",
      "CPU times: user 1d 2h 57min 53s, sys: 756 ms, total: 1d 2h 57min 54s\n",
      "Wall time: 1d 2h 57min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_dict = {'Extra neurons':[0, 1, 2, 3],  \n",
    "                'None':[],\n",
    "                'normal':[],\n",
    "                'uniform':[]}\n",
    "for init_form in [None, 'normal', 'uniform']:\n",
    "    print(f'{init_form} init')\n",
    "    for num_extra_neurons in results_dict['Extra neurons']:\n",
    "        print(f'    {num_extra_neurons} extra neurons')\n",
    "        _, counter = experiment_1(X, y, init_form=init_form,\n",
    "                                  num_extra_neurons=num_extra_neurons, num_runs=100)\n",
    "        print(f'    {counter}')\n",
    "        results_dict[str(init_form)].append(counter['success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc07fa5-fbaa-4792-98aa-5b93074fc809",
   "metadata": {},
   "source": [
    "# Результаты, выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9703239d-6c87-4b6b-b5ea-74a54697042f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extra neurons</th>\n",
       "      <th>None</th>\n",
       "      <th>normal</th>\n",
       "      <th>uniform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>55</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Extra neurons  None  normal  uniform\n",
       "0              0     0       0        0\n",
       "1              1    39      55       52\n",
       "2              2    88      85       85\n",
       "3              3    98      93       88"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0be141-33ed-4a82-99b5-dbbf8e83f54a",
   "metadata": {},
   "source": [
    "* Добавление всего одного экстра-нейрона, инициализированного со случайными весами, позволяет сети учиться. \n",
    "* Чем больше добавляем экстра-нейронов, тем более вероятно обучение завершится успехом.\n",
    "* При одном экстра-нейроне Ксавье-инициализация, что нормальная, что равномерная, повышает вероятность успешно обучиться. Но при большем числе экстра-нейронов, как будто снижает, если это можно считать статистически значимым, либо как минимум не повышает.\n",
    "* Тем не менее, думаю пока можно принять решение использовать **нормальную Ксавье-инициализацию**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4f698-bc6c-4588-bc53-f554de1c9975",
   "metadata": {},
   "source": [
    "# Эксперимент 2. Дают ли буст предобученные нейроны, или все дело только в архитектуре?\n",
    "Учим большую модель из 3 случайных нейронов и модель с 2 предобученными и 1 экстра"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a510eb0-2bda-49b0-952d-205fcf9b0c97",
   "metadata": {},
   "source": [
    "## Большая модель из 3 случайных скрытых нейронов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9d3c78a-cc61-495d-955c-4a584838fcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10000: loss 0.00\n",
      "epoch 20000: loss 0.00\n",
      "epoch 30000: loss 0.00\n",
      "epoch 40000: loss 0.00\n",
      "epoch 50000: loss 0.00\n",
      "epoch 60000: loss 0.00\n",
      "epoch 70000: loss 0.00\n",
      "epoch 80000: loss 0.00\n",
      "epoch 90000: loss 0.00\n",
      "epoch 100000: loss 0.00\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "neurons = [Neuron(n_inputs=n, activation=nn.Sigmoid()) for _ in range(3)]\n",
    "\n",
    "big_model = BigModel(neurons, activation=nn.Sigmoid()).to(device)\n",
    "\n",
    "train(big_model, X, y, device=device, epochs=100000, verbose=True)\n",
    "\n",
    "check(big_model, X, y, device=device)\n",
    "\n",
    "result_model = deepcopy(big_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50078910-9e33-42a9-b751-a79cc025ed9b",
   "metadata": {},
   "source": [
    "# Попытка 10 раз обучить модель с 3 случайными нейронами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d6511a4-b447-4bd1-a790-ac6c6a0b8201",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200: loss 0.65\n",
      "epoch 400: loss 0.23\n",
      "epoch 600: loss 0.08\n",
      "epoch 800: loss 0.04\n",
      "epoch 1000: loss 0.02\n",
      "epoch 1200: loss 0.02\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "Success!\n",
      "epoch 200: loss 0.63\n",
      "epoch 400: loss 0.20\n",
      "epoch 600: loss 0.08\n",
      "epoch 800: loss 0.05\n",
      "epoch 1000: loss 0.03\n",
      "epoch 1200: loss 0.02\n",
      "epoch 1400: loss 0.02\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "Success!\n",
      "epoch 200: loss 0.53\n",
      "epoch 400: loss 0.14\n",
      "epoch 600: loss 0.06\n",
      "epoch 800: loss 0.04\n",
      "epoch 1000: loss 0.03\n",
      "epoch 1200: loss 0.02\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "Success!\n",
      "epoch 200: loss 0.51\n",
      "epoch 400: loss 0.14\n",
      "epoch 600: loss 0.06\n",
      "epoch 800: loss 0.04\n",
      "epoch 1000: loss 0.03\n",
      "epoch 1200: loss 0.02\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "Success!\n",
      "epoch 200: loss 0.62\n",
      "epoch 400: loss 0.23\n",
      "epoch 600: loss 0.10\n",
      "epoch 800: loss 0.06\n",
      "epoch 1000: loss 0.04\n",
      "epoch 1200: loss 0.03\n",
      "epoch 1400: loss 0.02\n",
      "epoch 1600: loss 0.02\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "Success!\n",
      "epoch 200: loss 0.50\n",
      "epoch 400: loss 0.39\n",
      "epoch 600: loss 0.37\n",
      "epoch 800: loss 0.36\n",
      "epoch 1000: loss 0.35\n",
      "epoch 1200: loss 0.35\n",
      "epoch 1400: loss 0.35\n",
      "epoch 1600: loss 0.35\n",
      "epoch 1800: loss 0.35\n",
      "epoch 2000: loss 0.35\n",
      "BigModel gives wrong answers for samples [2]\n",
      "epoch 200: loss 0.34\n",
      "epoch 400: loss 0.10\n",
      "epoch 600: loss 0.05\n",
      "epoch 800: loss 0.03\n",
      "epoch 1000: loss 0.02\n",
      "epoch 1200: loss 0.01\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.00\n",
      "Success!\n",
      "epoch 200: loss 0.43\n",
      "epoch 400: loss 0.11\n",
      "epoch 600: loss 0.04\n",
      "epoch 800: loss 0.02\n",
      "epoch 1000: loss 0.01\n",
      "epoch 1200: loss 0.01\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.00\n",
      "epoch 2000: loss 0.00\n",
      "Success!\n",
      "epoch 200: loss 0.50\n",
      "epoch 400: loss 0.14\n",
      "epoch 600: loss 0.07\n",
      "epoch 800: loss 0.04\n",
      "epoch 1000: loss 0.03\n",
      "epoch 1200: loss 0.02\n",
      "epoch 1400: loss 0.02\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "Success!\n",
      "epoch 200: loss 0.44\n",
      "epoch 400: loss 0.11\n",
      "epoch 600: loss 0.05\n",
      "epoch 800: loss 0.03\n",
      "epoch 1000: loss 0.02\n",
      "epoch 1200: loss 0.02\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "Success!\n",
      "{'success': 9, 'fail': 1}\n"
     ]
    }
   ],
   "source": [
    "counter_3_random = {'success':0, 'fail':0}\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    #     !!!\n",
    "    neurons = [Neuron(n_inputs=n, activation=nn.Sigmoid()) for _ in range(3)]\n",
    "\n",
    "    big_model = BigModel(neurons, activation=nn.Sigmoid()).to(device)\n",
    "\n",
    "    train(big_model, X, y, device=device, epochs=2000, verbose=True)\n",
    "\n",
    "    X_new, y_new = check(big_model, X, y, device=device)\n",
    "\n",
    "    result_model = deepcopy(big_model)\n",
    "    \n",
    "    if X_new is None:\n",
    "        counter_3_random['success'] += 1\n",
    "    else:\n",
    "        counter_3_random['fail'] += 1\n",
    "\n",
    "print(counter_3_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5402d914-380e-4f4e-8bac-f0237a81a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200: loss 0.69\n",
      "epoch 400: loss 0.69\n",
      "epoch 600: loss 0.69\n",
      "epoch 800: loss 0.53\n",
      "epoch 1000: loss 0.49\n",
      "epoch 1200: loss 0.49\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [1]\n",
      "epoch 200: loss 0.56\n",
      "epoch 400: loss 0.50\n",
      "epoch 600: loss 0.49\n",
      "epoch 800: loss 0.49\n",
      "epoch 1000: loss 0.48\n",
      "epoch 1200: loss 0.48\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [1]\n",
      "epoch 200: loss 0.55\n",
      "epoch 400: loss 0.50\n",
      "epoch 600: loss 0.49\n",
      "epoch 800: loss 0.48\n",
      "epoch 1000: loss 0.48\n",
      "epoch 1200: loss 0.48\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [2]\n",
      "epoch 200: loss 0.69\n",
      "epoch 400: loss 0.51\n",
      "epoch 600: loss 0.49\n",
      "epoch 800: loss 0.48\n",
      "epoch 1000: loss 0.48\n",
      "epoch 1200: loss 0.48\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [3]\n",
      "epoch 200: loss 0.56\n",
      "epoch 400: loss 0.50\n",
      "epoch 600: loss 0.49\n",
      "epoch 800: loss 0.49\n",
      "epoch 1000: loss 0.48\n",
      "epoch 1200: loss 0.48\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [3]\n",
      "epoch 200: loss 0.68\n",
      "epoch 400: loss 0.54\n",
      "epoch 600: loss 0.50\n",
      "epoch 800: loss 0.49\n",
      "epoch 1000: loss 0.49\n",
      "epoch 1200: loss 0.48\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [3]\n",
      "epoch 200: loss 0.55\n",
      "epoch 400: loss 0.50\n",
      "epoch 600: loss 0.49\n",
      "epoch 800: loss 0.48\n",
      "epoch 1000: loss 0.48\n",
      "epoch 1200: loss 0.48\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [1]\n",
      "epoch 200: loss 0.55\n",
      "epoch 400: loss 0.50\n",
      "epoch 600: loss 0.49\n",
      "epoch 800: loss 0.48\n",
      "epoch 1000: loss 0.48\n",
      "epoch 1200: loss 0.48\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [3]\n",
      "epoch 200: loss 0.54\n",
      "epoch 400: loss 0.50\n",
      "epoch 600: loss 0.49\n",
      "epoch 800: loss 0.49\n",
      "epoch 1000: loss 0.48\n",
      "epoch 1200: loss 0.48\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [1]\n",
      "epoch 200: loss 0.56\n",
      "epoch 400: loss 0.51\n",
      "epoch 600: loss 0.49\n",
      "epoch 800: loss 0.49\n",
      "epoch 1000: loss 0.48\n",
      "epoch 1200: loss 0.48\n",
      "epoch 1400: loss 0.48\n",
      "epoch 1600: loss 0.48\n",
      "epoch 1800: loss 0.48\n",
      "epoch 2000: loss 0.48\n",
      "BigModel gives wrong answers for samples [2]\n",
      "{'success': 0, 'fail': 10}\n"
     ]
    }
   ],
   "source": [
    "counter_3_random_bad = {'success':0, 'fail':0}\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    #     !!!\n",
    "    neurons = [Neuron(n_inputs=n, activation=nn.Sigmoid())] * 3\n",
    "\n",
    "    big_model = BigModel(neurons, activation=nn.Sigmoid()).to(device)\n",
    "\n",
    "    train(big_model, X, y, device=device, epochs=2000, verbose=True)\n",
    "\n",
    "    X_new, y_new = check(big_model, X, y, device=device)\n",
    "\n",
    "    result_model = deepcopy(big_model)\n",
    "    \n",
    "    if X_new is None:\n",
    "        counter_3_random_bad['success'] += 1\n",
    "    else:\n",
    "        counter_3_random_bad['fail'] += 1\n",
    "\n",
    "print(counter_3_random_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d024d155-5269-4463-a746-d750a6e95ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995343191688\n",
      "1995343191688\n",
      "1995343191688\n",
      "Parameter containing:\n",
      "tensor([[ 9.0889, -8.3191]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 9.0889, -8.3191]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 9.0889, -8.3191]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.3849, -2.3189, -3.2634]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(id(result_model.base_models[0]))\n",
    "print(id(result_model.base_models[1]))\n",
    "print(id(result_model.base_models[2]))\n",
    "print(result_model.base_models[0].fc.weight)\n",
    "print(result_model.base_models[1].fc.weight)\n",
    "print(result_model.base_models[2].fc.weight)\n",
    "print(result_model.fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "691e4fb7-d949-486a-ab41-8f5ed997a65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200: loss 0.67\n",
      "epoch 400: loss 0.27\n",
      "epoch 600: loss 0.07\n",
      "epoch 800: loss 0.04\n",
      "epoch 1000: loss 0.02\n",
      "epoch 1200: loss 0.02\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "epoch 200: loss 0.54\n",
      "epoch 400: loss 0.38\n",
      "epoch 600: loss 0.36\n",
      "epoch 800: loss 0.35\n",
      "epoch 1000: loss 0.35\n",
      "epoch 1200: loss 0.35\n",
      "epoch 1400: loss 0.35\n",
      "epoch 1600: loss 0.35\n",
      "epoch 1800: loss 0.35\n",
      "epoch 2000: loss 0.35\n",
      "epoch 200: loss 0.56\n",
      "epoch 400: loss 0.37\n",
      "epoch 600: loss 0.36\n",
      "epoch 800: loss 0.35\n",
      "epoch 1000: loss 0.35\n",
      "epoch 1200: loss 0.35\n",
      "epoch 1400: loss 0.35\n",
      "epoch 1600: loss 0.35\n",
      "epoch 1800: loss 0.35\n",
      "epoch 2000: loss 0.35\n",
      "epoch 200: loss 0.68\n",
      "epoch 400: loss 0.33\n",
      "epoch 600: loss 0.14\n",
      "epoch 800: loss 0.07\n",
      "epoch 1000: loss 0.04\n",
      "epoch 1200: loss 0.03\n",
      "epoch 1400: loss 0.02\n",
      "epoch 1600: loss 0.02\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "epoch 200: loss 0.56\n",
      "epoch 400: loss 0.11\n",
      "epoch 600: loss 0.05\n",
      "epoch 800: loss 0.03\n",
      "epoch 1000: loss 0.02\n",
      "epoch 1200: loss 0.01\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.00\n",
      "epoch 200: loss 0.48\n",
      "epoch 400: loss 0.37\n",
      "epoch 600: loss 0.36\n",
      "epoch 800: loss 0.35\n",
      "epoch 1000: loss 0.35\n",
      "epoch 1200: loss 0.35\n",
      "epoch 1400: loss 0.35\n",
      "epoch 1600: loss 0.35\n",
      "epoch 1800: loss 0.35\n",
      "epoch 2000: loss 0.35\n",
      "epoch 200: loss 0.52\n",
      "epoch 400: loss 0.37\n",
      "epoch 600: loss 0.36\n",
      "epoch 800: loss 0.35\n",
      "epoch 1000: loss 0.35\n",
      "epoch 1200: loss 0.35\n",
      "epoch 1400: loss 0.35\n",
      "epoch 1600: loss 0.35\n",
      "epoch 1800: loss 0.35\n",
      "epoch 2000: loss 0.35\n",
      "epoch 200: loss 0.64\n",
      "epoch 400: loss 0.38\n",
      "epoch 600: loss 0.36\n",
      "epoch 800: loss 0.35\n",
      "epoch 1000: loss 0.35\n",
      "epoch 1200: loss 0.35\n",
      "epoch 1400: loss 0.35\n",
      "epoch 1600: loss 0.35\n",
      "epoch 1800: loss 0.35\n",
      "epoch 2000: loss 0.35\n",
      "epoch 200: loss 0.56\n",
      "epoch 400: loss 0.11\n",
      "epoch 600: loss 0.04\n",
      "epoch 800: loss 0.02\n",
      "epoch 1000: loss 0.01\n",
      "epoch 1200: loss 0.01\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.00\n",
      "epoch 2000: loss 0.00\n",
      "epoch 200: loss 0.56\n",
      "epoch 400: loss 0.16\n",
      "epoch 600: loss 0.06\n",
      "epoch 800: loss 0.03\n",
      "epoch 1000: loss 0.02\n",
      "epoch 1200: loss 0.01\n",
      "epoch 1400: loss 0.01\n",
      "epoch 1600: loss 0.01\n",
      "epoch 1800: loss 0.01\n",
      "epoch 2000: loss 0.01\n",
      "{'success': 5, 'fail': 5}\n"
     ]
    }
   ],
   "source": [
    "counter = {'success':0, 'fail':0}\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    neurons = []\n",
    "    current_neuron = Neuron(n_inputs=n, activation=nn.Sigmoid()).to(device)\n",
    "\n",
    "    train(current_neuron, X, y, device=device, epochs=5000, verbose=False)\n",
    "\n",
    "    X_new, y_new = check(current_neuron, X, y, device=device, verbose=False)\n",
    "\n",
    "    if X_new is None:\n",
    "        result_model = current_neuron\n",
    "        print(result_model)\n",
    "\n",
    "    else:\n",
    "        neurons.append(deepcopy(current_neuron))\n",
    "\n",
    "        while X_new is not None:\n",
    "\n",
    "            current_neuron = Neuron(n_inputs=n, activation=nn.Sigmoid()).to(device)\n",
    "\n",
    "            train(current_neuron, X_new, y_new, device=device, epochs=5000, verbose=False)\n",
    "\n",
    "            X_new, y_new = check(current_neuron, X_new, y_new, device=device, verbose=False)\n",
    "\n",
    "            neurons.append(deepcopy(current_neuron))\n",
    "\n",
    "        for extra in range(1):\n",
    "            neurons.append(Neuron(n_inputs=n, activation=nn.Sigmoid()).to(device))\n",
    "\n",
    "        big_model = BigModel(neurons, activation=nn.Sigmoid()).to(device)\n",
    "\n",
    "        train(big_model, X, y, device=device, epochs=2000, verbose=True)\n",
    "\n",
    "        X_new, y_new = check(big_model, X, y, device=device, verbose=False)\n",
    "\n",
    "        result_model = deepcopy(big_model)\n",
    "\n",
    "    if X_new is None:\n",
    "        counter['success'] += 1\n",
    "    else:\n",
    "        counter['fail'] += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b466b50-f4a7-4a7a-9db3-b98eff4bcf65",
   "metadata": {},
   "source": [
    "## Результаты, выводы\n",
    "Была ошибка в инициализации большой модели из 3-х случайных нейронов: код \n",
    "\n",
    "` neurons = [Neuron(n_inputs=n, activation=nn.Sigmoid())] * 3`\n",
    "\n",
    "`big_model = BigModel(neurons, activation=nn.Sigmoid()).to(device)`\n",
    "\n",
    "задает модель из трех нейронов, которые *были ссылками на один и тот же объект*. В `neurons` было 3 нейрона, но это было 3 копии одного объекта. И тогда модель не училась (0 успехов из 10 запусков).\n",
    "\n",
    "Если делать правильно \n",
    "\n",
    "` neurons = [Neuron(n_inputs=n, activation=nn.Sigmoid()) for _ in range(3)]`\n",
    "\n",
    "То это будет 3 честных случайных независимых нейрона. В таком случае модель учится в 9 случаев из 10.\n",
    "\n",
    "А вот использование 2-х предобученных, и дополнительного одного случайного, в тех же условиях обучения, приводит лишь к 5 успешным обучениям из 10.\n",
    "\n",
    "В итоге можно сделать вывод, что обучение с нуля даже лучше, а использование двух предобученных нейронов в совокупности с экстра случайным, только мешает.\n",
    "\n",
    ":( :( :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
