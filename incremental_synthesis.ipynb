{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ec5eb2-3cbe-4652-9d78-2bb72d039795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82e93260-1815-4f14-8723-b6ce10d3ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bin(x, n=7):\n",
    "    \"\"\"\n",
    "    creating a binary list of integer non-negative x\n",
    "    \"\"\"\n",
    "    u = 2 ** n - 1\n",
    "    x = int(x)\n",
    "    assert x >= 0, 'Input value x must be non-negative'\n",
    "    assert x <= u, f'Input value x with n = {n} must be less than {u}'\n",
    "    \n",
    "    y = []\n",
    "    if x == 0:\n",
    "        for i in range(n):\n",
    "            y.append(0)\n",
    "    else:\n",
    "        while x != 1:\n",
    "            y.append(x % 2)\n",
    "            x = x // 2\n",
    "        y.append(x)\n",
    "        delta = n - len(y)\n",
    "        for i in range(delta):\n",
    "            y.append(0)\n",
    "        y.reverse()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db837d9c-04b2-41e5-8ee4-e615959b4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_X(n_inputs):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    all binary combinations of these variables\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for i in range(2**n_inputs):\n",
    "        X += [dec2bin(i, n=n_inputs)]\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40a2cedd-519d-4d14-bce2-4e612aa608d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_boolfunc(n_inputs):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    the truth table, where output is random binary vector\n",
    "    \"\"\"\n",
    "    X = get_all_X(n_inputs)\n",
    "    y = np.random.randint(0, 2, size=(2**n_inputs, 1))\n",
    "    return X, y\n",
    "   \n",
    "def _and(n_inputs=2):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    the truth table of AND logical gate\n",
    "    \"\"\"\n",
    "    X = get_all_X(n_inputs)\n",
    "    y = np.append(\n",
    "        np.zeros(shape=(2 ** n_inputs - 1, 1), dtype=int),\n",
    "        np.array([[1]]),\n",
    "        axis=0\n",
    "    )\n",
    "    return X, y\n",
    "    \n",
    "def _or(n_inputs=2):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    the truth table of OR logical gate\n",
    "    \"\"\"\n",
    "    X = get_all_X(n_inputs)\n",
    "    y = np.append(\n",
    "        np.array([[0]]),\n",
    "        np.ones(shape=(2 ** n_inputs - 1, 1), dtype=int),\n",
    "        axis=0\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "def _xor(n_inputs=2):\n",
    "    \"\"\"\n",
    "    for given number of variables returns\n",
    "    the truth table of XOR logical gate\n",
    "    \"\"\"\n",
    "    X = get_all_X(n_inputs)\n",
    "    y = (np.sum(X, axis=1) % 2).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def to_dataframe(X, y):\n",
    "    \"\"\"\n",
    "    for the truth table in form of two arrays \n",
    "    X [2 ** n_inputs, n_inputs] and y [2 ** n_inputs, 1]\n",
    "    combine it to the form of Pandas DataFrame\n",
    "    \"\"\"\n",
    "    data=np.concatenate((X, y), axis=1)\n",
    "    n_inputs = X.shape[1]\n",
    "    return pd.DataFrame(data=data, columns=[f'x{i}' for i in range(n_inputs, 0, -1)] + ['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f95844a9-9975-4863-9d3d-f1f3707fdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79d825d5-cecf-4112-ade4-df71d38a5f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 1.]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "X, y = random_boolfunc(n_inputs=n)\n",
    "X, y = torch.Tensor(X), torch.Tensor(y)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9865343b-c317-4dcb-99d7-40f05e809384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 1.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.]]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = _xor()\n",
    "X, y = torch.Tensor(X), torch.Tensor(y)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1415d0e8-0837-47da-b1b7-ce2121b95b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(nn.Module):\n",
    "    def __init__(self, n_inputs, activation=nn.Sigmoid()):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 1)\n",
    "        #self.fc2 = nn.Linear(2,1)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8076a7fc-9e0b-48ab-9a75-e3c99bb7429b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: loss 0.69\n",
      "epoch 20: loss 0.69\n",
      "epoch 30: loss 0.69\n",
      "epoch 40: loss 0.69\n",
      "epoch 50: loss 0.69\n",
      "epoch 60: loss 0.69\n",
      "epoch 70: loss 0.69\n",
      "epoch 80: loss 0.69\n",
      "epoch 90: loss 0.69\n",
      "epoch 100: loss 0.69\n",
      "epoch 110: loss 0.69\n",
      "epoch 120: loss 0.69\n",
      "epoch 130: loss 0.69\n",
      "epoch 140: loss 0.69\n",
      "epoch 150: loss 0.69\n",
      "epoch 160: loss 0.69\n",
      "epoch 170: loss 0.69\n",
      "epoch 180: loss 0.69\n",
      "epoch 190: loss 0.69\n",
      "epoch 200: loss 0.69\n",
      "epoch 210: loss 0.69\n",
      "epoch 220: loss 0.69\n",
      "epoch 230: loss 0.69\n",
      "epoch 240: loss 0.69\n",
      "epoch 250: loss 0.69\n",
      "epoch 260: loss 0.69\n",
      "epoch 270: loss 0.69\n",
      "epoch 280: loss 0.69\n",
      "epoch 290: loss 0.69\n",
      "epoch 300: loss 0.69\n",
      "epoch 310: loss 0.69\n",
      "epoch 320: loss 0.69\n",
      "epoch 330: loss 0.69\n",
      "epoch 340: loss 0.69\n",
      "epoch 350: loss 0.69\n",
      "epoch 360: loss 0.69\n",
      "epoch 370: loss 0.69\n",
      "epoch 380: loss 0.69\n",
      "epoch 390: loss 0.69\n",
      "epoch 400: loss 0.69\n",
      "epoch 410: loss 0.69\n",
      "epoch 420: loss 0.69\n",
      "epoch 430: loss 0.69\n",
      "epoch 440: loss 0.69\n",
      "epoch 450: loss 0.69\n",
      "epoch 460: loss 0.69\n",
      "epoch 470: loss 0.69\n",
      "epoch 480: loss 0.69\n",
      "epoch 490: loss 0.69\n",
      "epoch 500: loss 0.69\n",
      "epoch 510: loss 0.69\n",
      "epoch 520: loss 0.69\n",
      "epoch 530: loss 0.69\n",
      "epoch 540: loss 0.69\n",
      "epoch 550: loss 0.69\n",
      "epoch 560: loss 0.69\n",
      "epoch 570: loss 0.69\n",
      "epoch 580: loss 0.69\n",
      "epoch 590: loss 0.69\n",
      "epoch 600: loss 0.69\n",
      "epoch 610: loss 0.69\n",
      "epoch 620: loss 0.69\n",
      "epoch 630: loss 0.69\n",
      "epoch 640: loss 0.69\n",
      "epoch 650: loss 0.69\n",
      "epoch 660: loss 0.69\n",
      "epoch 670: loss 0.69\n",
      "epoch 680: loss 0.69\n",
      "epoch 690: loss 0.69\n",
      "epoch 700: loss 0.69\n",
      "epoch 710: loss 0.69\n",
      "epoch 720: loss 0.69\n",
      "epoch 730: loss 0.69\n",
      "epoch 740: loss 0.69\n",
      "epoch 750: loss 0.69\n",
      "epoch 760: loss 0.69\n",
      "epoch 770: loss 0.69\n",
      "epoch 780: loss 0.69\n",
      "epoch 790: loss 0.69\n",
      "epoch 800: loss 0.69\n",
      "epoch 810: loss 0.69\n",
      "epoch 820: loss 0.69\n",
      "epoch 830: loss 0.69\n",
      "epoch 840: loss 0.69\n",
      "epoch 850: loss 0.69\n",
      "epoch 860: loss 0.69\n",
      "epoch 870: loss 0.69\n",
      "epoch 880: loss 0.69\n",
      "epoch 890: loss 0.69\n",
      "epoch 900: loss 0.69\n",
      "epoch 910: loss 0.69\n",
      "epoch 920: loss 0.69\n",
      "epoch 930: loss 0.69\n",
      "epoch 940: loss 0.69\n",
      "epoch 950: loss 0.69\n",
      "epoch 960: loss 0.69\n",
      "epoch 970: loss 0.69\n",
      "epoch 980: loss 0.69\n",
      "epoch 990: loss 0.69\n",
      "epoch 1000: loss 0.69\n"
     ]
    }
   ],
   "source": [
    "model = Neuron(n_inputs=n)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
    "criterion = torch.nn.BCELoss()\n",
    "#criterion = torch.nn.MSELoss()\n",
    "n_epochs = 1000\n",
    "for i in range(1, n_epochs+1):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print(f'epoch {i}: loss {loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57cb4d-8559-4c14-a9ba-fcc4ad287edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "323a6d6f-5ea1-4dd7-934e-47c59c66ff6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5000],\n",
       "         [0.5000],\n",
       "         [0.5000],\n",
       "         [0.5000]], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.]]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "da412e14-5af5-4ddf-ac57-be99be5c4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X[[0,3]]\n",
    "y_new = y[[0,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8812ebf7-2473-489b-98ff-fd08080aab24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "07229066-7fd3-4a62-9794-3de127efeb76",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: loss 0.25\n",
      "epoch 20: loss 0.11\n",
      "epoch 30: loss 0.07\n",
      "epoch 40: loss 0.04\n",
      "epoch 50: loss 0.03\n",
      "epoch 60: loss 0.03\n",
      "epoch 70: loss 0.02\n",
      "epoch 80: loss 0.02\n",
      "epoch 90: loss 0.02\n",
      "epoch 100: loss 0.01\n",
      "epoch 110: loss 0.01\n",
      "epoch 120: loss 0.01\n",
      "epoch 130: loss 0.01\n",
      "epoch 140: loss 0.01\n",
      "epoch 150: loss 0.01\n",
      "epoch 160: loss 0.01\n",
      "epoch 170: loss 0.01\n",
      "epoch 180: loss 0.01\n",
      "epoch 190: loss 0.01\n",
      "epoch 200: loss 0.01\n",
      "epoch 210: loss 0.01\n",
      "epoch 220: loss 0.00\n",
      "epoch 230: loss 0.00\n",
      "epoch 240: loss 0.00\n",
      "epoch 250: loss 0.00\n",
      "epoch 260: loss 0.00\n",
      "epoch 270: loss 0.00\n",
      "epoch 280: loss 0.00\n",
      "epoch 290: loss 0.00\n",
      "epoch 300: loss 0.00\n",
      "epoch 310: loss 0.00\n",
      "epoch 320: loss 0.00\n",
      "epoch 330: loss 0.00\n",
      "epoch 340: loss 0.00\n",
      "epoch 350: loss 0.00\n",
      "epoch 360: loss 0.00\n",
      "epoch 370: loss 0.00\n",
      "epoch 380: loss 0.00\n",
      "epoch 390: loss 0.00\n",
      "epoch 400: loss 0.00\n",
      "epoch 410: loss 0.00\n",
      "epoch 420: loss 0.00\n",
      "epoch 430: loss 0.00\n",
      "epoch 440: loss 0.00\n",
      "epoch 450: loss 0.00\n",
      "epoch 460: loss 0.00\n",
      "epoch 470: loss 0.00\n",
      "epoch 480: loss 0.00\n",
      "epoch 490: loss 0.00\n",
      "epoch 500: loss 0.00\n",
      "epoch 510: loss 0.00\n",
      "epoch 520: loss 0.00\n",
      "epoch 530: loss 0.00\n",
      "epoch 540: loss 0.00\n",
      "epoch 550: loss 0.00\n",
      "epoch 560: loss 0.00\n",
      "epoch 570: loss 0.00\n",
      "epoch 580: loss 0.00\n",
      "epoch 590: loss 0.00\n",
      "epoch 600: loss 0.00\n",
      "epoch 610: loss 0.00\n",
      "epoch 620: loss 0.00\n",
      "epoch 630: loss 0.00\n",
      "epoch 640: loss 0.00\n",
      "epoch 650: loss 0.00\n",
      "epoch 660: loss 0.00\n",
      "epoch 670: loss 0.00\n",
      "epoch 680: loss 0.00\n",
      "epoch 690: loss 0.00\n",
      "epoch 700: loss 0.00\n",
      "epoch 710: loss 0.00\n",
      "epoch 720: loss 0.00\n",
      "epoch 730: loss 0.00\n",
      "epoch 740: loss 0.00\n",
      "epoch 750: loss 0.00\n",
      "epoch 760: loss 0.00\n",
      "epoch 770: loss 0.00\n",
      "epoch 780: loss 0.00\n",
      "epoch 790: loss 0.00\n",
      "epoch 800: loss 0.00\n",
      "epoch 810: loss 0.00\n",
      "epoch 820: loss 0.00\n",
      "epoch 830: loss 0.00\n",
      "epoch 840: loss 0.00\n",
      "epoch 850: loss 0.00\n",
      "epoch 860: loss 0.00\n",
      "epoch 870: loss 0.00\n",
      "epoch 880: loss 0.00\n",
      "epoch 890: loss 0.00\n",
      "epoch 900: loss 0.00\n",
      "epoch 910: loss 0.00\n",
      "epoch 920: loss 0.00\n",
      "epoch 930: loss 0.00\n",
      "epoch 940: loss 0.00\n",
      "epoch 950: loss 0.00\n",
      "epoch 960: loss 0.00\n",
      "epoch 970: loss 0.00\n",
      "epoch 980: loss 0.00\n",
      "epoch 990: loss 0.00\n",
      "epoch 1000: loss 0.00\n"
     ]
    }
   ],
   "source": [
    "model2 = Neuron(n_inputs=n)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr = 0.1)\n",
    "criterion = torch.nn.BCELoss()\n",
    "#criterion = torch.nn.MSELoss()\n",
    "n_epochs = 1000\n",
    "for i in range(1, n_epochs+1):\n",
    "        optimizer.zero_grad()\n",
    "        pred2 = model2(X_new)\n",
    "        loss = criterion(pred2, y_new)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print(f'epoch {i}: loss {loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b963c09f-e196-4df8-9996-063181b36d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9.6581e-04],\n",
       "         [1.2281e-05]], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.],\n",
       "         [0.]]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "505a5de5-e733-45f8-8901-451dba62a62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 6.5637e-08, -3.5102e-08]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.0935e-07], requires_grad=True)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "230a2a2a-eb09-4135-acd4-a7135bd07ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "print(*[1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9826ce83-002f-42d7-949c-14b03f9c4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.logic.boolalg import ANFform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "08182d41-d101-44fd-8b2b-9fb5a16e14c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x \\veebar \\left(\\text{True}\\right)$"
      ],
      "text/plain": [
       "x ^ True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy.logic.boolalg import ANFform\n",
    "from sympy.abc import x, y, z\n",
    "ANFform([x], [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f802fc2b-1be8-4357-9918-c2620dc6e0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x \\veebar y \\veebar \\left(x \\wedge y\\right)$"
      ],
      "text/plain": [
       "x ^ y ^ (x & y)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANFform([x, y], [0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1eab6a88-ed85-4eb6-a742-1b6e71993663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y \\veebar z \\veebar \\left(x \\wedge y\\right) \\veebar \\left(y \\wedge z\\right) \\veebar \\left(\\text{True}\\right)$"
      ],
      "text/plain": [
       "y ^ z ^ True ^ (x & y) ^ (y & z)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANFform([x, y, z], [1, 0, 0, 0, 1, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "03e16afa-2404-4baf-94c0-0b7df7d511b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy.logic.boolalg import anf_coeffs, bool_monomial, Xor\n",
    "from sympy.abc import a, b, c\n",
    "truthvalues = [int(i) for i in list('10001011')]\n",
    "coeffs = anf_coeffs(truthvalues)\n",
    "coeffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c4094-2a3e-49b4-800c-816c18d35a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b38ad799-f5cd-4290-8a04-132fcd766015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\neg \\left(b \\veebar c \\veebar \\left(a \\wedge b\\right) \\veebar \\left(b \\wedge c\\right)\\right)$"
      ],
      "text/plain": [
       "~(b ^ c ^ (a & b) ^ (b & c))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial = Xor(*[\n",
    "    bool_monomial(k, [a, b, c])\n",
    "    for k, coeff in enumerate(coeffs) if coeff == 1\n",
    "])\n",
    "polynomial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
